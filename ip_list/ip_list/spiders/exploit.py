import scrapy
from datetime import datetime
import os
from os.path import dirname

current_dir = os.path.dirname(__file__)
url = os.path.join(current_dir, 'source-EXPLOIT-DB.html')
top_dir = dirname(dirname(dirname(current_dir)))
sql_file = os.path.join(top_dir, 'sql_files/populate.sql')


class ExploitSpider(scrapy.Spider):
    name = 'exploit'
    allowed_domains = ['cybercrime-tracker.net']
    start_urls = ['https://cybercrime-tracker.net/fuckerz.php/']

    def parse(self, response):
        for row in response.xpath('//*[@class="ExploitTable"]//tbody//tr'):
            try:
                ip = row.xpath('td//text()')[0].extract()
                Country = row.xpath('td//img/@title')[0].extract()
                print("{} - {}".format(ip, Country))
                append_sql_file(ip,Country)    
            except Exception as err:
                print(f"skipping due to: {err}")


def append_sql_file(ip, Country):
    line = f"INSERT INTO badips(ip, Country) VALUES ('{ip}', '{str(Country)}');\n"
    if not os.path.exists(sql_file):
        with open(sql_file, 'w') as _f:
            _f.write(line)
        return
    with open(sql_file, 'a') as _f:
        _f.write(line)
